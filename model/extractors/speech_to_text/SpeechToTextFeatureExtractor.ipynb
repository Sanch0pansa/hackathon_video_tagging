{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from moviepy.editor import VideoFileClip\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import os\n",
    "import wave\n",
    "\n",
    "class SpeechToTextFeatureExtractor:\n",
    "    def __init__(self, embedding_model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", vosk_model_path=\"path/to/vosk/russian/model\"):\n",
    "        # Embedding model (changed to multilingual model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name, cache_dir=\"./cache\")\n",
    "        self.model = AutoModel.from_pretrained(embedding_model_name, cache_dir=\"./cache\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Vosk setup (use Russian model)\n",
    "        self.vosk_model = Model(vosk_model_path)\n",
    "\n",
    "    def extract_audio(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = video.audio\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        audio.write_audiofile(audio_path, codec='pcm_s16le')\n",
    "        return audio_path\n",
    "\n",
    "    def audio_to_text(self, audio_path):\n",
    "        # Convert audio to text using Vosk\n",
    "        wf = wave.open(audio_path, \"rb\")\n",
    "        rec = KaldiRecognizer(self.vosk_model, wf.getframerate())\n",
    "\n",
    "        text = \"\"\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                result = json.loads(rec.Result())\n",
    "                text += result.get(\"text\", \"\") + \" \"\n",
    "\n",
    "        final_result = json.loads(rec.FinalResult())\n",
    "        text += final_result.get(\"text\", \"\")\n",
    "        return text.strip()\n",
    "\n",
    "    def get_embeddings(self, text):\n",
    "        # Get embeddings from text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\",\n",
    "                                padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "\n",
    "    def extract_features(self, video_path):\n",
    "        # Extract features from video\n",
    "        audio_path = self.extract_audio(video_path)\n",
    "        text = self.audio_to_text(audio_path)\n",
    "        embeddings = self.get_embeddings(text)\n",
    "\n",
    "        # Clean up temporary audio file\n",
    "        os.remove(audio_path)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test block:\n",
    "english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/graph/HCLr.fst /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: /home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/0a148a3aa95e76ced2d993525badc986.mp4\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   9%|▉         | 9778/108839 [00:02<00:28, 3448.22it/s, now=None]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract audio\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_video_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert audio to text with progress bar\u001b[39;00m\n\u001b[1;32m     18\u001b[0m wf \u001b[38;5;241m=\u001b[39m wave\u001b[38;5;241m.\u001b[39mopen(audio_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m, in \u001b[0;36mSpeechToTextFeatureExtractor.extract_audio\u001b[0;34m(self, video_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m audio \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39maudio\n\u001b[1;32m     24\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_audio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_audiofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpcm_s16le\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio_path\n",
      "File \u001b[0;32m<decorator-gen-63>:2\u001b[0m, in \u001b[0;36mwrite_audiofile\u001b[0;34m(self, filename, fps, nbytes, buffersize, codec, bitrate, ffmpeg_params, write_logfile, verbose, logger)\u001b[0m\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.11/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.11/site-packages/moviepy/audio/AudioClip.py:206\u001b[0m, in \u001b[0;36mAudioClip.write_audiofile\u001b[0;34m(self, filename, fps, nbytes, buffersize, codec, bitrate, ffmpeg_params, write_logfile, verbose, logger)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoviePy couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find the codec associated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith the filename. Provide the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter in write_audiofile.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mffmpeg_audiowrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffersize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcodec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mwrite_logfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_logfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-27>:2\u001b[0m, in \u001b[0;36mffmpeg_audiowrite\u001b[0;34m(clip, filename, fps, nbytes, buffersize, codec, bitrate, write_logfile, verbose, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.11/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.11/site-packages/moviepy/audio/io/ffmpeg_audiowriter.py:170\u001b[0m, in \u001b[0;36mffmpeg_audiowrite\u001b[0;34m(clip, filename, fps, nbytes, buffersize, codec, bitrate, write_logfile, verbose, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    161\u001b[0m writer \u001b[38;5;241m=\u001b[39m FFMPEG_AudioWriter(filename, fps, nbytes, clip\u001b[38;5;241m.\u001b[39mnchannels,\n\u001b[1;32m    162\u001b[0m                             codec\u001b[38;5;241m=\u001b[39mcodec, bitrate\u001b[38;5;241m=\u001b[39mbitrate,\n\u001b[1;32m    163\u001b[0m                             logfile\u001b[38;5;241m=\u001b[39mlogfile,\n\u001b[1;32m    164\u001b[0m                             ffmpeg_params\u001b[38;5;241m=\u001b[39mffmpeg_params)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m clip\u001b[38;5;241m.\u001b[39miter_chunks(chunksize\u001b[38;5;241m=\u001b[39mbuffersize,\n\u001b[1;32m    167\u001b[0m                               quantize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m                               nbytes\u001b[38;5;241m=\u001b[39mnbytes, fps\u001b[38;5;241m=\u001b[39mfps,\n\u001b[1;32m    169\u001b[0m                               logger\u001b[38;5;241m=\u001b[39mlogger):\n\u001b[0;32m--> 170\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_logfile:\n",
      "File \u001b[0;32m~/Code/.venv/lib/python3.11/site-packages/moviepy/audio/io/ffmpeg_audiowriter.py:74\u001b[0m, in \u001b[0;36mFFMPEG_AudioWriter.write_frames\u001b[0;34m(self, frames_array)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(frames_array\u001b[38;5;241m.\u001b[39mtobytes())\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(frames_array\u001b[38;5;241m.\u001b[39mtostring())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   9%|▉         | 9893/108839 [00:19<00:28, 3448.22it/s, now=None]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an instance of the extractor\n",
    "extractor = SpeechToTextFeatureExtractor(vosk_model_path=\"/home/glooma/.cache/vosk/vosk-model-small-en-us-0.15\")\n",
    "\n",
    "# Path to the test video\n",
    "# short video\n",
    "test_video_path = \"/home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\"\n",
    "# long video\n",
    "# test_video_path = \"/home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/0a148a3aa95e76ced2d993525badc986.mp4\"\n",
    "print(f\"Processing video: {test_video_path}\")\n",
    "\n",
    "# Extract audio\n",
    "audio_path = extractor.extract_audio(test_video_path)\n",
    "\n",
    "# Convert audio to text with progress bar\n",
    "wf = wave.open(audio_path, \"rb\")\n",
    "rec = KaldiRecognizer(extractor.vosk_model, wf.getframerate())\n",
    "\n",
    "text = \"\"\n",
    "total_frames = wf.getnframes()\n",
    "chunk_size = 4000\n",
    "progress_bar = tqdm(total=total_frames, unit='frames', desc=\"Processing audio\")\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(chunk_size)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result = json.loads(rec.Result())\n",
    "        text += result.get(\"text\", \"\") + \" \"\n",
    "    progress_bar.update(len(data))\n",
    "\n",
    "final_result = json.loads(rec.FinalResult())\n",
    "text += final_result.get(\"text\", \"\")\n",
    "text = text.strip()\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Transcribed text: {text[:100]}...\")  # Print first 100 characters\n",
    "\n",
    "# Get embeddings from the transcribed text\n",
    "embeddings = extractor.get_embeddings(text)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First few values of embeddings: {embeddings[0][:5]}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(audio_path):\n",
    "    os.remove(audio_path)\n",
    "    print(f\"Temporary audio file removed: {audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test block\n",
    "russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/graph/HCLr.fst /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка видео: /home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка аудио: 5115600фреймы [00:08, 570691.79фреймы/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Транскрибированный текст: группу юту кода стоишь не руках это было слышно смешной и стоишь у порах ну  ну да   мы уже дорога ч...\n",
      "Форма эмбеддингов: torch.Size([1, 384])\n",
      "Первые несколько значений эмбеддингов: tensor([ 0.0685,  0.0816, -0.1150, -0.2033, -0.0663])\n",
      "Временный аудиофайл удален: temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import wave\n",
    "import json\n",
    "from vosk import KaldiRecognizer  # Add this import\n",
    "\n",
    "\n",
    "# Путь к русской модели Vosk\n",
    "vosk_model_path = \"/home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22\"\n",
    "\n",
    "# Проверка существования модели\n",
    "if not os.path.exists(vosk_model_path):\n",
    "    print(f\"Модель не найдена по пути: {vosk_model_path}\")\n",
    "    print(\"Пожалуйста, скачайте модель с https://alphacephei.com/vosk/models\")\n",
    "    print(\"и распакуйте ее в указанную директорию.\")\n",
    "    raise Exception(\"Модель Vosk не найдена\")\n",
    "\n",
    "# Create an instance of the extractor\n",
    "extractor = SpeechToTextFeatureExtractor(vosk_model_path=vosk_model_path)\n",
    "\n",
    "# Path to the test video\n",
    "# short video\n",
    "test_video_path = \"/home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\"\n",
    "# long video\n",
    "# test_video_path = \"/home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/0a148a3aa95e76ced2d993525badc986.mp4\"\n",
    "\n",
    "print(f\"Обработка видео: {test_video_path}\")\n",
    "\n",
    "# Extract audio\n",
    "audio_path = extractor.extract_audio(test_video_path)\n",
    "\n",
    "# Convert audio to text with progress bar\n",
    "wf = wave.open(audio_path, \"rb\")\n",
    "rec = KaldiRecognizer(extractor.vosk_model, wf.getframerate())  # Changed this line\n",
    "\n",
    "text = \"\"\n",
    "total_frames = wf.getnframes()\n",
    "chunk_size = 4000\n",
    "progress_bar = tqdm(total=total_frames, unit='фреймы', desc=\"Обработка аудио\")\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(chunk_size)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        result = json.loads(rec.Result())\n",
    "        text += result.get(\"text\", \"\") + \" \"\n",
    "    progress_bar.update(len(data))\n",
    "\n",
    "final_result = json.loads(rec.FinalResult())\n",
    "text += final_result.get(\"text\", \"\")\n",
    "text = text.strip()\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Транскрибированный текст: {text[:100]}...\")  # Вывод первых 100 символов\n",
    "\n",
    "# Get embeddings from the transcribed text\n",
    "embeddings = extractor.get_embeddings(text)\n",
    "print(f\"Форма эмбеддингов: {embeddings.shape}\")\n",
    "print(f\"Первые несколько значений эмбеддингов: {embeddings[0][:5]}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(audio_path):\n",
    "    os.remove(audio_path)\n",
    "    print(f\"Временный аудиофайл удален: {audio_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
