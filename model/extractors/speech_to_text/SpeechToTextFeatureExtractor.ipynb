{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from moviepy.editor import VideoFileClip\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import json\n",
    "import os\n",
    "import wave\n",
    "\n",
    "class SpeechToTextFeatureExtractor:\n",
    "    def __init__(self, embedding_model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", vosk_model_path=\"path/to/vosk/russian/model\"):\n",
    "        # Embedding model (changed to multilingual model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name, cache_dir=\"./cache\")\n",
    "        self.model = AutoModel.from_pretrained(embedding_model_name, cache_dir=\"./cache\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "        # Vosk setup (use Russian model)\n",
    "        self.vosk_model = Model(vosk_model_path)\n",
    "\n",
    "    def extract_audio(self, video_path):\n",
    "        # Extract audio from video\n",
    "        video = VideoFileClip(video_path)\n",
    "        audio = video.audio\n",
    "        audio_path = \"temp_audio.wav\"\n",
    "        audio.write_audiofile(audio_path, codec='pcm_s16le')\n",
    "        return audio_path\n",
    "\n",
    "    def audio_to_text(self, audio_path):\n",
    "        # Convert audio to text using Vosk\n",
    "        wf = wave.open(audio_path, \"rb\")\n",
    "        rec = KaldiRecognizer(self.vosk_model, wf.getframerate())\n",
    "\n",
    "        text = \"\"\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                result = json.loads(rec.Result())\n",
    "                text += result.get(\"text\", \"\") + \" \"\n",
    "\n",
    "        final_result = json.loads(rec.FinalResult())\n",
    "        text += final_result.get(\"text\", \"\")\n",
    "        return text.strip()\n",
    "\n",
    "    def get_embeddings(self, text):\n",
    "        # Get embeddings from text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\",\n",
    "                                padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "\n",
    "    def extract_features(self, video_path):\n",
    "        # Extract features from video\n",
    "        audio_path = self.extract_audio(video_path)\n",
    "        text = self.audio_to_text(audio_path)\n",
    "        embeddings = self.get_embeddings(text)\n",
    "\n",
    "        # Clean up temporary audio file\n",
    "        os.remove(audio_path)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test block:\n",
    "english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/graph/HCLr.fst /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/glooma/.cache/vosk/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: /home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Transcribed text: q able to little too you don't kid institutionally close to those two snooty co solution go on the w...\n",
      "Embeddings shape: torch.Size([1, 384])\n",
      "First few values of embeddings: tensor([-0.0732, -0.0688, -0.1041,  0.2425, -0.0169])\n",
      "Temporary audio file removed: temp_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the extractor\n",
    "extractor = SpeechToTextFeatureExtractor(vosk_model_path=\"/home/glooma/.cache/vosk/vosk-model-small-en-us-0.15\")\n",
    "\n",
    "# Path to the test video\n",
    "test_video_path = \"/home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\"\n",
    "\n",
    "print(f\"Processing video: {test_video_path}\")\n",
    "\n",
    "# Extract audio\n",
    "audio_path = extractor.extract_audio(test_video_path)\n",
    "\n",
    "# Convert audio to text\n",
    "text = extractor.audio_to_text(audio_path)\n",
    "print(f\"Transcribed text: {text[:100]}...\")  # Print first 100 characters\n",
    "\n",
    "# Get embeddings from the transcribed text\n",
    "embeddings = extractor.get_embeddings(text)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First few values of embeddings: {embeddings[0][:5]}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(audio_path):\n",
    "    os.remove(audio_path)\n",
    "    print(f\"Temporary audio file removed: {audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test block\n",
    "russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/graph/HCLr.fst /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:308) Loading winfo /home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработка видео: /home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\n",
      "MoviePy - Writing audio in temp_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Транскрибированный текст: группу юту кода стоишь не руках это было слышно смешной и стоишь у порах ну  ну да   мы уже дорога ч...\n",
      "Форма эмбеддингов: torch.Size([1, 384])\n",
      "Первые несколько значений эмбеддингов: tensor([ 0.0685,  0.0816, -0.1150, -0.2033, -0.0663])\n",
      "Временный аудиофайл удален: temp_audio.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Путь к русской модели Vosk\n",
    "vosk_model_path = \"/home/glooma/Code/Python/ML/Hakatons/hackathon_video_tagging/model/extractors/speech_to_text/vosk-model-small-ru-0.22\"\n",
    "\n",
    "# Проверка существования модели\n",
    "if not os.path.exists(vosk_model_path):\n",
    "    print(f\"Модель не найдена по пути: {vosk_model_path}\")\n",
    "    print(\"Пожалуйста, скачайте модель с https://alphacephei.com/vosk/models\")\n",
    "    print(\"и распакуйте ее в указанную директорию.\")\n",
    "    raise Exception(\"Модель Vosk не найдена\")\n",
    "\n",
    "# Create an instance of the extractor\n",
    "extractor = SpeechToTextFeatureExtractor(vosk_model_path=vosk_model_path)\n",
    "\n",
    "# Path to the test video\n",
    "test_video_path = \"/home/glooma/Code/Python/ML/Hakatons/train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\"\n",
    "\n",
    "print(f\"Обработка видео: {test_video_path}\")\n",
    "\n",
    "# Extract audio\n",
    "audio_path = extractor.extract_audio(test_video_path)\n",
    "\n",
    "# Convert audio to text\n",
    "text = extractor.audio_to_text(audio_path)\n",
    "print(f\"Транскрибированный текст: {text[:100]}...\")  # Print first 100 characters\n",
    "\n",
    "# Get embeddings from the transcribed text\n",
    "embeddings = extractor.get_embeddings(text)\n",
    "print(f\"Форма эмбеддингов: {embeddings.shape}\")\n",
    "print(f\"Первые несколько значений эмбеддингов: {embeddings[0][:5]}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "if os.path.exists(audio_path):\n",
    "    os.remove(audio_path)\n",
    "    print(f\"Временный аудиофайл удален: {audio_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
