{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Embeddings from text\n",
    "1) using Bert pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Model BERT text to embeddings\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=\"./cache\")\n",
    "        self.model = BertModel.from_pretrained(model_name, output_hidden_states=True, cache_dir=\"./cache\")\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(device)\n",
    "    \n",
    "    def extract_features(self, text: str) -> torch.Tensor:\n",
    "        # Tokenize input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Extract embeddings from the layer\n",
    "        embeddings = outputs.hidden_states[-2]\n",
    "\n",
    "        # Average embeddings of all tokens\n",
    "        sentence_embedding = torch.mean(embeddings, dim=1)\n",
    "        \n",
    "        return sentence_embedding\n",
    "\n",
    "    def cosine_similarity(self, a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.cosine_similarity(a, b, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for feature extraction below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: torch.Size([1, 768])\n",
      "Sum of absolute values of the first embedding: 298.31231689453125\n",
      "Mean value of the first embedding: -0.036628805100917816\n",
      "Similarity between text 1 and 2 (both about pasta): 0.967228889465332\n",
      "Similarity between text 1 and 4 (pasta and car): 0.9320670366287231\n",
      "Similarity between two embeddings of the same text: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "texts = [\n",
    "    \"Рецепты итальянской пасты.  Различные блюда из пасты. \",\n",
    "    \"Спагетти карбонара. Классическая римская паста.\",\n",
    "    \"Техники приготовления пасты. Как сделать свежую пасту.\",\n",
    "    \"Советы по уходу за автомобилем. Основной уход за автомобилем.\",\n",
    "    \"Замена спущенного колеса. Пошаговое руководство.\",\n",
    "    \"Основы ремонта автомобилей. Ремонт автомобилей своими руками.\"\n",
    "]\n",
    "\n",
    "extractor = TextFeatureExtractor()\n",
    "embeddings = [extractor.extract_features(text) for text in texts]\n",
    "\n",
    "print(f\"Embedding dimension: {embeddings[0].shape}\")\n",
    "\n",
    "# Check non-zero vectors\n",
    "print(f\"Sum of absolute values of the first embedding: {torch.sum(torch.abs(embeddings[0]))}\")\n",
    "print(f\"Mean value of the first embedding: {torch.mean(embeddings[0])}\")\n",
    "\n",
    "print(f\"Similarity between text 1 and 2 (both about pasta): {extractor.cosine_similarity(embeddings[0], embeddings[1]).item()}\")\n",
    "print(f\"Similarity between text 1 and 4 (pasta and car): {extractor.cosine_similarity(embeddings[0], embeddings[3]).item()}\")\n",
    "\n",
    "# Check consistency\n",
    "embedding1 = extractor.extract_features(texts[0])\n",
    "embedding2 = extractor.extract_features(texts[0])\n",
    "print(f\"Similarity between two embeddings of the same text: {extractor.cosine_similarity(embedding1, embedding2).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real usage\n",
    "text - input information\n",
    "\n",
    "embeddings - output information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 768])\n",
      "Embedding mean: -0.037250421941280365\n",
      "Embedding sum: -28.60832405090332\n",
      "Embedding max: 1.4767484664916992\n",
      "Embedding min: -5.696453094482422\n"
     ]
    }
   ],
   "source": [
    "extractor = TextFeatureExtractor()\n",
    "\n",
    "text = \"Искусство приготовления пиццы. Секреты идеального теста и соуса.\"\n",
    "\n",
    "embeddings = extractor.extract_features(text)\n",
    "\n",
    "# some statistics of the embeddings\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding mean: {torch.mean(embeddings)}\")\n",
    "print(f\"Embedding sum: {torch.sum(embeddings)}\")\n",
    "print(f\"Embedding max: {torch.max(embeddings)}\")\n",
    "print(f\"Embedding min: {torch.min(embeddings)}\")\n",
    "\n",
    "# real full info about embeddings: \n",
    "# attention: \"kinda big output\"\n",
    "\n",
    "# print(f\"Embedding values: {embeddings}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
