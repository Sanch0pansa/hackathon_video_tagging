{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Embeddings from video\n",
    "## Using VideoMAE pretrained model\n",
    "Model allow us to provide only 16 frames, so we'll catch evenly distributed frames based on total frames count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "\n",
    "class VideoFeatureExtractor:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize the image processor and model\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\", cache_dir=\"./cache\")\n",
    "        self.model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\", cache_dir=\"./cache\")\n",
    "\n",
    "        # Determine the device (GPU or CPU)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def _preprocess_frames(self, frames):\n",
    "        \"\"\"\n",
    "        Apply preprocessing to each frame using AutoImageProcessor.\n",
    "        Resize frames to the required size and normalize them.\n",
    "        \"\"\"\n",
    "        # Resize frames and return tensors\n",
    "        inputs = self.image_processor(frames, return_tensors=\"pt\", size=(224, 224))\n",
    "        return inputs\n",
    "\n",
    "    def _video_to_frames(self, video_path, target_frame_count):\n",
    "        \"\"\"\n",
    "        Split video into frames using OpenCV.\n",
    "        Dynamically select the number of frames based on video length and the required frame count.\n",
    "        \"\"\"\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total number of frames in the video\n",
    "\n",
    "        # Calculate how many frames to skip for target_frame_count\n",
    "        frame_skip = max(1, total_frames / target_frame_count)\n",
    "\n",
    "        frames = []\n",
    "        count = 0\n",
    "        success, image = vidcap.read()\n",
    "        while success:\n",
    "            if count > frame_skip * len(frames):\n",
    "                # Resize the frame using OpenCV if necessary\n",
    "                image_resized = cv2.resize(image, (224, 224))\n",
    "                frames.append(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))\n",
    "            success, image = vidcap.read()\n",
    "            count += 1\n",
    "\n",
    "        vidcap.release()\n",
    "\n",
    "        # If the number of selected frames is less than target_frame_count,\n",
    "        # apply padding (add empty frames)\n",
    "        while len(frames) < target_frame_count:\n",
    "            frames.append(torch.zeros((224, 224, 3), dtype=torch.uint8).numpy())  # Add empty frame\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def extract_features(self, video_path: str, target_frame_count: int = 16) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract features using the VideoMAE model.\n",
    "        Dynamically select the number of frames based on video length.\n",
    "        \"\"\"\n",
    "        frames = self._video_to_frames(video_path, target_frame_count)\n",
    "        \n",
    "        if not frames:\n",
    "            raise ValueError(\"Failed to extract frames from video\")\n",
    "\n",
    "        # Preprocess the frames and resize to 224x224\n",
    "        inputs = self._preprocess_frames(frames)\n",
    "        \n",
    "        # Move tensors to GPU or CPU\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Extract features using the model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Get embeddings\n",
    "        embeddings = outputs.last_hidden_state  # or another output layer if needed\n",
    "        return embeddings.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for feature extraction below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define extractor\n",
    "fe = VideoFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../../train_dataset_tag_video/videos/1f17968167d4b0487cdebb6a67b4f148.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3ec7c2b092514dc4ebeaa3036fe9857c.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0c069f42ac98970c28d471d615e71f7b.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2c5bdce3e9e2c8b9db713d9f2c196820.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3b69f98d51c1028633cff24c7d2937e0.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1e922b7daeef2358f82b263533d450ac.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1e244ec5c0c85f9478a83695ac9add45.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0ac7ed0507b2364e40030d11bf52ee5d.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/02d4cc029c5a7531f36992446a24478f.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/4ae45fdfd387f5a00225798709fe0337.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3a84cf5e54a272f77470dd066f554edb.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0e0cc6672f469d6800b53017c1296f60.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1c6bc481dd52a9938e78e755f1e5c90e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3de3fdfefe2ccf8ee2a55980ac660a32.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/02ee1613c4b59ee0f885fda0186d2429.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2f841d8ea7c9850785024d7f5baf842d.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1b8c1e0f941d6d58368ca7956f097f14.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3cd4675b5f7795057e8bea311dd14b9a.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3e6ed0e674aee621b7e9560d195d6ddc.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0e22ec9e51bd98309b4fe6870c2ce9c8.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0a7a288165c6051ebd74010be4dc9aa8.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/01dcabe37935dbbffd389c22c327f361.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0dade416613c9961614dfb4513ff4461.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3b1309649ff64b03b77f9e6925fa458f.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2e4c7dae0c99f47839880e2da7fc6769.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0b7834cc1bb493636600674074345998.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3a6fbaefd25a49db80c8d8db21f1c1d7.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1f4038d52c0388419fffc0004204106e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0f790686916874c2e5ca0382f4276bce.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/02b797508b9ffc941887703dfb17d365.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/4ac3ae3413347e4bcaa66faf649e5cde.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0f88b5afbb77a356de0b7567713149b3.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0e171c8779313353eab104bdae49acc6.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3dc231d648df3dfd18b109131be791ca.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1a5743706b4355451991676db508deeb.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2ef3ec7442459323bfb283992ffe001f.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0a1bd95990029433d45dd0a7b15d9b50.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/00efa58930724f2ae6f9916f53cda3b3.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/4a6d6e98413508867dc1adafef8c5e46.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3f6e4aa060e1a873a199e35bedb0916d.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0ac24affcaad658a5bbe26650192069f.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2eb66cbfde5fd77eb83a95ae24d68d21.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3d4f700d85dbc7724d66a54bdd17bbde.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3b23b8507c5cec618d524225353ab118.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2a4e2848527b622b82996bd2136dd913.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2a3881f30716e778b7808502f1c27498.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1dbc2732cedac20ab19cf7dbc17ce3b8.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2dc87cc0bbb8275d1870da2f33cbdb12.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3d1ea59037afbbe33d7c7b64f1e4f7c2.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2af42e54e82e38e1f0ff49c3a30d1f8c.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/4a0c52aebf8915dfcb6826b7dbfd1ce5.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1ec391b3afbbb417a13d3e0db926faa7.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2dd6cb41f7aa5eb0fd1f21fe361957bf.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0d5137eb3278b721a353749a75252860.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0d0cb2e712923b3bace3320beec9476e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2c32ef7b31d84f080c86e7278cb510f6.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2f4bd0532525895de06ac9411287cc33.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2e57018a4e5282ee4378f4ae7d412682.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3bb037f137368bea0b35524aad530e42.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0e2a1403a7bb53e14188ccb05bedd14a.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3d09581a8dc3029466120aefbf610315.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0db9125eb3a43a903066546eff25331f.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0d5b866fe07382f31b8668f0d1dafdf9.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2d3c2aaf3ae0c94a04ec31cb7c384817.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0f73958b7266131eb83707f0f4850c65.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3fc81df4bfe121ce2bc33dd581f5efeb.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0e99ef446324a7db9272c3335004190f.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1bd67fa7af55c97961534de842e3945e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2d256d3ba0a68aeeb3699263750f2bd5.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0ac1817349ccfc9cd7252a95e22eb087.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1fc3cbd9d4c0255ad64b407a4e813521.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1c78f72bd8c56515486ad6a1eb464d7d.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2f6be0a89ed03d5ad2a91165d728322e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0bbdcd822239ea2d32c0768364854fbb.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2d2b45bb4f4129749721c7f377a93de6.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0f0c6bdc92c5e36dd49883ab3622b52c.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3cbdf1a7d2c4600866b2ee7acef454d9.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1ad4048ab1da9df744715a8a99eac8b8.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0a00bb871aec241e7065ed9cb26acfc4.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0ea016128113476c741eba66ecbb5f0a.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1a9595342c12cdaa9c6589b98f1281cf.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3e73c40dca86b8e0b02503b57c8ea8b1.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1e0a5151efc26a3a8e038e132f6b80f4.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2e5e72cbea39eecffee4a175b1ce22fd.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0a148a3aa95e76ced2d993525badc986.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1c0258e20430ce9defb4e0f68b9b5d88.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3c870fcc822206e9f0882fb70744ed14.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/01bd6b00e670655ca7ff484ff55d8ad9.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0b73cea7b2dfdd7048dd84b95f8b9b0e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0c2c9f1343ba94ec3a28a131bdff8067.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1ea9081352700dd78ee4efae44f905db.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0d7b83b4d141158214acf42ab2ca0519.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1eed890f769711c1a21300fa3db33334.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1a291fbe33c55a983b08f5fa60f71710.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0fa2c2b0938c3b5e3ce9b191e455f2d6.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3f3a30c842dc7a468245b86b30c530d0.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2a67205d8725698ef55feee26dac4c4a.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3fb2a25286fa29be587f8812d09c6491.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0cee2037e28ed045c425a6dbeca1bff2.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3a8760f7de25a13acc15ffb30081f13a.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3e5597709ea64a34496d4ac047dfcb04.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1db856cf0ef82b11a4b85859a1ef0167.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2b8fcc124b15a2fa9440b86bed5a9ee3.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3bf2e36c3963425263babe36b8dbd740.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3c562522e2fff930245e02270570209e.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2cf3241929c3a2e03d76cb90b6f7740b.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0cf954675163c4fe8f1313b6e6bb8c19.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2ae34024d1a38e2a869d64520ed79f20.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/3ec26b67015f2dbcb8addddd2e8ffab2.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0d1a9fa41857d58c005ccde4ca517685.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0db157b2a0b65848184bf65e3cb2a57d.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/0c6e4cdaa192d1ae58b99bc9f35891b9.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/2e4eca888ca4bb3fc09f967fcc500eb2.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/1d0f431c41252a7fac46f8f499b7bebc.mp4\n",
      "torch.Size([1, 768])\n",
      "processing ../../train_dataset_tag_video/videos/4a4dc53857b44464613052a331877a07.mp4\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Iterate videos\n",
    "p = Path(\"../../train_dataset_tag_video/videos\")\n",
    "videos = [x for x in p.iterdir() if str(x).endswith(\".mp4\")]\n",
    "results = []\n",
    "for v in videos:\n",
    "    print(\"processing\", str(v))\n",
    "    res = fe.extract_features(str(v))\n",
    "    print(res.shape)\n",
    "    results.append(\n",
    "        (\n",
    "            res,\n",
    "            v\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding most similar videos based on embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar videos: \n",
      "\t../../train_dataset_tag_video/videos/2dc87cc0bbb8275d1870da2f33cbdb12.mp4\n",
      "\t../../train_dataset_tag_video/videos/3a8760f7de25a13acc15ffb30081f13a.mp4\n",
      "Most dissimilar videos: \n",
      "\t../../train_dataset_tag_video/videos/0c069f42ac98970c28d471d615e71f7b.mp4\n",
      "\t../../train_dataset_tag_video/videos/3bf2e36c3963425263babe36b8dbd740.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "\n",
    "# Найдем все пары тензоров\n",
    "pairs = list(itertools.combinations(results, 2))\n",
    "\n",
    "min_sim = float('inf')\n",
    "max_sim = float('-inf')\n",
    "most_similar = None\n",
    "most_dissimilar = None\n",
    "\n",
    "for (tensor1, path1), (tensor2, path2) in pairs:\n",
    "    if str(path1) != str(path2):\n",
    "        sim = F.cosine_similarity(tensor1, tensor2)\n",
    "        \n",
    "        if sim < min_sim:\n",
    "            min_sim = sim\n",
    "            most_dissimilar = ((tensor1, path1), (tensor2, path2))\n",
    "        \n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            most_similar = ((tensor1, path1), (tensor2, path2))\n",
    "\n",
    "# Результаты\n",
    "print(f\"Most similar videos: \\n\\t{str(most_similar[0][1])}\\n\\t{str(most_similar[1][1])}\")\n",
    "print(f\"Most dissimilar videos: \\n\\t{str(most_dissimilar[0][1])}\\n\\t{str(most_dissimilar[1][1])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
